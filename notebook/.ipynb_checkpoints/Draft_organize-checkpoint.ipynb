{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T02:45:41.149620Z",
     "start_time": "2020-09-28T02:45:41.132631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import inflection\n",
    "import dateparser\n",
    "import pyxlsb\n",
    "import warnings\n",
    "import datetime as t\n",
    "from datetime import time \n",
    "\n",
    "#Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "#Possible Displays\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Helper Functions\n",
    "In this section, we define some functions that will help us along the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T02:45:44.152279Z",
     "start_time": "2020-09-28T02:45:44.106308Z"
    }
   },
   "outputs": [],
   "source": [
    "# sets seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "# sets matplolit inline\n",
    "%matplotlib inline\n",
    "\n",
    "# sets to display unlimited number of columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# setting the title and axis labels\n",
    "def set_plots_features(ax, title, xlabel, ylabel):\n",
    "    ax.set_title(title, fontsize=18)\n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    \n",
    "# ignores warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "def get_descriptive_statistics(data_set):\n",
    "    # central tendency: mean, median\n",
    "    mean = pd.DataFrame(data_set.apply(np.mean)).T\n",
    "    median = pd.DataFrame(data_set.apply(np.median)).T\n",
    "    \n",
    "    # distribution: std, min, max, range, skew, kurtosis\n",
    "    std = pd.DataFrame(data_set.apply(np.std)).T\n",
    "    min_value = pd.DataFrame(data_set.apply(min)).T\n",
    "    max_value = pd.DataFrame(data_set.apply(max)).T\n",
    "    range_value = pd.DataFrame(data_set.apply(lambda x: x.max() - x.min())).T\n",
    "    skewness = pd.DataFrame(data_set.apply(lambda x: x.skew())).T\n",
    "    kurtosis = pd.DataFrame(data_set.apply(lambda x: x.kurtosis())).T\n",
    "\n",
    "    # concatenates\n",
    "    summary_stats = pd.concat(\n",
    "            [min_value, max_value, range_value, mean, median, std, skewness, kurtosis]).T.reset_index()\n",
    "    summary_stats.columns = ['attributes', 'min', 'max',\n",
    "                                 'range', 'mean', 'median', 'std', 'skewness', 'kurtosis']\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter configuring to better experience\n",
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "        \n",
    "    plt.style.use('bmh')\n",
    "    plt.rcParams['figure.figsize'] = [25,9]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "    display( HTML('<style>.container{width:100% !important; }</style>'))\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    \n",
    "    sns.set()\n",
    "jupyter_settings()  \n",
    "\n",
    "# To hide warning messages\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2  Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2.1 -- Business Case – Guidelines\n",
    "\n",
    "    You are overseeing Data Analytics for one contact center project. You arrive to the office one day, and the contact center manager calls you about near past results. The client believes CSAT is one of the most important metrics (customers are everything for them) and has been complaining as we were not able, in 2018, to keep a good CSAT – in fact CSAT has been decreasing finding the minimum in December – and wanted to know what happened and which actions we will take for the future. \n",
    "    \n",
    "      As a Data Analytics expert, your job is to figure out why we couldn’t achieve good results (previously it has been poorly diagnosed as any action plan worked) and to propose an action plan that can prevent such results from happening again. \n",
    "      \n",
    " ![image.png](../img/01_csat.png)\n",
    " \n",
    "      As a guideline, here follows the main metrics, targets and formulas:\n",
    "      \n",
    "#### • Answer rate - 92% - #AnsweredCalls/ OfferedCalls\n",
    "#### • CSAT - 85% - #Surveys8to10/#Surveys\n",
    "#### • DSAT - 8% - #Surveys1to3/#Surveys\n",
    "#### • SLA email - 85% - #AnsweredLess24H/ #Answered \n",
    "      \n",
    "      Additionally, propose a report template to ensure visibility over key metrics of the project to support stakeholders’ future control & decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Understanding the Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3.1 -- Why ? \n",
    "   #### -- What is the type of business of Teleperformance? - \n",
    "     Outsourcing and Technology. They mission is to provide customer experience excellency at each interaction opportunity\n",
    "     \n",
    "   ![image.png](../img/02_htech-htouch.png)\n",
    "   \n",
    "   #### -- Whats the focuses of the company ? \n",
    "     The company has three well-defined focuses: customer, innovation and efficiency. \n",
    "   #### -- Offer ? (Solutions for Customers)\n",
    "           -- E-mail \n",
    "           -- Calls \n",
    "           \n",
    "   #### -- Enviroment - Call Center\n",
    "   #### -- Target - Increase Service Level Agreement (SLA)\n",
    "   #### -- Goals - Find deviations in the procedures used to resolve customer issues.\n",
    "           1) Deliverys \n",
    "           a) Exploratory Data Analysis \n",
    "           b) Insights for a better customer experience\n",
    "           c) Power BI Dashboard\n",
    "           d) ML Algorithm for predict the next calls occurrences in the next month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Understanding the Dataset\n",
    "This part of the problem is essential!\n",
    "Try to understand the nature of the columns (What they mean) and then establish the granularity, and know what is essential or not, based on the metrics previously established in 0.2\n",
    "\n",
    "                            The first step is UNDERSTAND some important ACRONYMS\n",
    "                            but the most part of the columns are auto-explained.\n",
    "                            In order to understand the importance of each one and thevalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Loading Data\n",
    "We have an Xlsb file with 5 tabs to be analyzed.\n",
    "The argument sheet_name make us possible to extract each tab.\n",
    "Then we gonna save all the tabs in individual csv files for be able to manipulate one by one, after this process we can select each of them to merge and continue analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awr = pd.read_excel('../data/Case_Study_Data.xlsb', engine='pyxlsb',sheet_name=['FACT HSPLIT',\n",
    "                                                                                 'FACT HAGENT', \n",
    "                                                                                 'FACT SERVREQ', \n",
    "                                                                                 'FACT EMAIL', \n",
    "                                                                                 'CSAT'])\n",
    "\n",
    "# FACT HSPLIT\n",
    "df_raw_tb1 = df_raw['FACT HSPLIT']\n",
    "#FACT HAGENT\n",
    "df_raw_tb2 = df_raw['FACT HAGENT']\n",
    "# FACT SERVREQ\n",
    "df_raw_tb3 = df_raw['FACT SERVREQ']\n",
    "# FACT EMAIL\n",
    "df_raw_tb4 = df_raw['FACT EMAIL']\n",
    "# CSAT\n",
    "df_raw_tb5 = df_raw['CSAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CSV FOR SAVE\n",
    "#FACT HSPLIT\n",
    "df_raw_tab1.to_excel('../data/FACT_HSPLIT.xlsx', index=False)\n",
    "#FACT HAGENT\n",
    "df_raw_tab2.to_excel('../data/FACT_HAGENT.xlsx', index=False)\n",
    "#FACT SERVREQ\n",
    "df_raw_tab3.to_excel('../data/FACT_SERVREQ.xlsx', index=False)\n",
    "#FACT EMAIL\n",
    "df_raw_tab4.to_excel('../data/FACT_EMAIL.xlsx', index_label=False)\n",
    "#CSAT\n",
    "df_raw_tab5.to_excel('../data/CSAT.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_tb1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5.2 Project Initial Checkpoint and Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_r.copy()\n",
    "\n",
    "df_tb1 = df_raw_tb1.copy()\n",
    "df_tb2 = df_raw_tb2.copy()\n",
    "df_tb3 = df_raw_tb3.copy()\n",
    "df_tb4 = df_raw_tb4.copy()\n",
    "df_tb5 = df_raw_tb5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Understanding the Dataset\n",
    "We already have an idea of ​​what we believe to be more important and which are not necessarily self-explanatory at the checkpoint: at this stage we will try to understand the data we have before actually cleaning and manipulating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* FACT HSPLIT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the spreadsheets info\n",
    "df_tb1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see that most part of the columns, meaning :\n",
    "   - DIM CALENDAR DATE\n",
    "   - DIM TIME HOUR\n",
    "   - DIM TIME. MINUTES\n",
    "   - DIM TIME ID (Maybe)\n",
    "   - AFTER TIME - HSPLIT\n",
    "   - ANSWER TIME - HSPLIT\n",
    "   - HANDLE TIME - IDLE TIME\n",
    "   - TALK TIME\n",
    "   - TIME)\n",
    "   \n",
    "They are related to TIME. We can already see that the dtypes are incorrect, which needs precisely a transformation.\n",
    "\n",
    "\n",
    "The columns\n",
    "(HOLD CALLS - HSPLIT\n",
    "HOLD TIME - HSPLIT\n",
    "I AUXTIME - HSPLIT)\n",
    "They are filled from the beginning to the end with the value 0. We will try to find out what this means and find out if we treat them as NaN Values ​​or as values ​​that we can drop from the dataset for a more precise analysis.\n",
    "\n",
    "*FACT HAGENT - df_tb2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Check Each Tab(Spreadsheet) of the DataSet \n",
    "df_tb2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FACT SERVREQ * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Columns overview\n",
    "          SHORT LOGIN - ND# - NULL-VALUES\n",
    "          DIM AGENT.LOGIN\t- GROUP BY - \n",
    "          DIM CALENDAR.DATE.1\t- DATE\n",
    "          Resolution Name\t\n",
    "          DIM TIME.HOUR\t\n",
    "          DIM TIME.MINUTES\t\n",
    "          DIM TIME.TIME ID\t\n",
    "          Incidents Created\t\n",
    "          Incidents Updated\t\n",
    "          Time\t\n",
    "          Tier - Group By and see whats its subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     For this we have to look how SLA is  builded \n",
    "       - Date \n",
    "       - Time Since arrived to close\n",
    "       - SLA ? \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will need to verify well, because apparently the metrics established in the Business case proposal do not match those of the DataSet in terms of numbers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACT HSPLIT\n",
    "\n",
    "cols_old_tb1 = ['DIM CALENDAR.DATE.1', 'DIM TIME.HOUR', 'DIM TIME.MINUTES',\n",
    "       'DIM TIME.TIME ID', 'AFTERCALL TIME - HSPLIT', 'ANSWER TIME - HSPLIT',\n",
    "       'CALLS ABANDONED - HSPLIT', 'Answered Calls', 'Offered Calls',\n",
    "       'HANDLE TIME - HSPLIT', 'HOLD CALLS - HSPLIT', 'HOLD TIME - HSPLIT',\n",
    "       'I AUXTIME - HSPLIT', 'IDLE TIME - HSPLIT', 'MAX DELAY - HSPLIT',\n",
    "       'STAFF TIME - HSPLIT', 'TALK TIME - HSPLIT', 'Time', 'Type',\n",
    "       'SKill ID']\n",
    "\n",
    "# snake_case\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "# creates new columns from old columns in snakecase \n",
    "\n",
    "cols_new_tb1 = list(map(snakecase, cols_old_tb1))\n",
    "\n",
    "# renames the old columns\n",
    "df_tb1.columns = cols_new_tb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACT HAGENT\n",
    "\n",
    "\n",
    "cols_old_tb2 = ['Short Login', 'DIM CALENDAR.DATE.1', 'CALLS ABANDONED',\n",
    "       'CALLS ANSWERED', 'CALLS CONFERENCED',\n",
    "       'CALLS RETURN TO Q DUE TO TIMEOUT', 'CALLS TRANSFERRED',\n",
    "       'CONTACT HOLDTIME', 'CONTACT TALK TIME', 'IDLE TIME', 'LOGIN DURATION',\n",
    "       'POST CALL PROCESSING TIME', 'RING TIME', 'SCHEDULED TIME',\n",
    "       'TOTAL STAFFED TIME', 'WAIT TIME', 'WORKED TIME', 'Type', 'Skill ID',\n",
    "       'LOB']\n",
    "\n",
    "# snake_case\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "# creates new columns from old columns in snakecase \n",
    "\n",
    "cols_new_tb2 = list(map(snakecase, cols_old_tb2))\n",
    "\n",
    "# renames the old columns\n",
    "df_tb2.columns = cols_new_tb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT SERVREQ\n",
    "\n",
    "cols_old_tb3 = ['DIM AGENT.LOGIN', 'DIM CALENDAR.DATE.1', 'Resolution Name',\n",
    "       'DIM TIME.HOUR', 'DIM TIME.MINUTES', 'DIM TIME.TIME ID',\n",
    "       'Incidents Created', 'Incidents Updated', 'Time', 'Tier']\n",
    "\n",
    "# snake_case\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "# creates new columns from old columns in snakecase \n",
    "\n",
    "cols_new_tb3 = list(map(snakecase, cols_old_tb3))\n",
    "\n",
    "# renames the old columns\n",
    "df_tb3.columns = cols_new_tb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT EMAIL\n",
    "\n",
    "cols_old_tb4 = ['Agent Login', 'Closed Reason', 'Avg. Time Allocated',\n",
    "       'AVG_TIME_ARRIVE_TO_CLOSE', 'SLA', 'COUNT ARRIVAL', 'COUNT CLOSED',\n",
    "       'COUNT FIRST OPENED', 'COUNT OPENED', 'TIME SINCE ARRIVED TO CLOSE',\n",
    "       'Date', 'Emails within SLA']\n",
    "\n",
    "# snake_case\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "# creates new columns from old columns in snakecase \n",
    "\n",
    "cols_new_tb4 = list(map(snakecase, cols_old_tb4))\n",
    "\n",
    "# renames the old columns\n",
    "df_tb4.columns = cols_new_tb4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSAT\n",
    "\n",
    "cols_old_tb5 = ['Agent Login', 'DIM CSAT PRODUCT LINE.NAME', 'Time to feedback',\n",
    "       'CONVERSION_RATE_CSAT', 'CONVERSION_RATE_DSAT', 'Number of CSAT',\n",
    "       'Number of DSAT', 'Number of Incidents', 'System Name', 'Tier', 'Date',\n",
    "       'FIRST_CALL_RESOLUTION', 'RESOLUTION_RATE', 'Cases resolved',\n",
    "       'Cases resolves - first res', 'Service', 'Channel']\n",
    "\n",
    "# snake_case\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "# creates new columns from old columns in snakecase \n",
    "\n",
    "cols_new_tb5 = list(map(snakecase, cols_old_tb5))\n",
    "\n",
    "# renames the old columns\n",
    "df_tb5.columns = cols_new_tb5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Checking Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT HSPLIT\n",
    "print('Number of rows : {}'.format(df_tb1.shape[0]))\n",
    "print('Number of cols : {}'.format(df_tb1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT HAGENT\n",
    "print('Number of rows : {}'.format(df_tb2.shape[0]))\n",
    "print('Number of cols : {}'.format(df_tb2.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT SERVREQ\n",
    "print('Number of rows : {}'.format(df_tb3.shape[0]))\n",
    "print('Number of cols : {}'.format(df_tb3.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT EMAIL \n",
    "print('Number of rows : {}'.format(df_tb4.shape[0]))\n",
    "print('Number of cols : {}'.format(df_tb4.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSAT\n",
    "print('Number of rows : {}'.format(df_tb2.shape[0]))\n",
    "print('Number of cols : {}'.format(df_tb2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Checking Data Types\n",
    "WE already have the DataTypes on the info! But let's see in a careful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT HSPLIT\n",
    "df_tb1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACT HAGENT\n",
    "df_tb2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT SERVREQ\n",
    "df_tb3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACT EMAIL\n",
    "df_tb4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSAT\n",
    "df_tb5.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Checking NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT HSPLIT\n",
    "df_tb1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT HAGENT\n",
    "df_tb2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT SERVREQ\n",
    "df_tb3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT EMAIL\n",
    "df_tb4.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSAT\n",
    "df_tb5.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Filling out NaN Values\n",
    "\n",
    " We need approuch the problem of NaN values in DF_TB4 in 2 ways:\n",
    "That means we gonna drop columns with nan values values, and drop the rows with NaN values\n",
    "\n",
    "We noticed that we have a lot of missing data. We can chosse one of the following three paths:\n",
    "\n",
    "1) Exclude all lines that have missing data, but we may lose important information that the EDA   may need and negatively impact the Analysis\n",
    "\n",
    "2) Use ML techniques to fill in the missing data and predict what value would replace the NaN. This method is good when we don't have business information available.\n",
    "\n",
    "3) Really understand the business. Understand the business rules and fill in the missing data with relevant information.\n",
    "\n",
    "        In the sense that we have non-zero values ​​in the columns. The approuch for this moment will be to drop the lines with null values ​​due to their absence affecting the type of information we want to extract.\n",
    "        Otherwise, ML Algorithm for this project it's a plus in consideration of what the challeng asked. If we have time, treat this values in a careful way will be/maybe necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FACT E-MAIL ROWS DROP\n",
    "df_tb4 = df_tb4.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the shape\n",
    "df_tb4.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb5 = df_tb5.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb5.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's see in the near future if this was a better choice *\n",
    "\n",
    "For now lets save the DataFrames without NaN values for work with them in Dashboard later, without have these task to fill NaN's again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO CSV FOR SAVE\n",
    "#FACT HSPLIT - No Nulls\n",
    "#FACT HAGENT - No Nulls\n",
    "#FACT SERVREQ - No Nulls\n",
    "\n",
    "#FACT EMAIL\n",
    "'''df_tb4.to_excel('../data/NoNaN/FACT_EMAIL_NN.xlsx', index_label=False)\n",
    "#CSAT\n",
    "df_tb5.to_excel('../data/NoNaN/CSAT_NN.xlsx', index=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Project Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_tb1 = df_tb1.copy()\n",
    "df1_tb2 = df_tb2.copy()\n",
    "df1_tb3 = df_tb3.copy()\n",
    "df1_tb4 = df_tb4.copy()\n",
    "df1_tb5 = df_tb5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Manipulation \n",
    "#### Changing Data Types & Drop Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.1 FACT HSPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACT HSPLIT\n",
    "# 1) Dim Calendar.date - TRANSFORMING IN DATE\n",
    "\n",
    "#1º We need to create a dictionarie for replace the name of the months for number\n",
    "months = {'janeiro':'-01-', 'fevereiro':'-02-', 'março':'-03-', 'abril':'-04-', 'maio':'-05-', 'junho':'-06-', 'julho':'-07-',\n",
    "         'agosto': '-08-', 'setembro':'-09-', 'outubro':'-10-', 'novembro':'-11-', 'dezembro':'-12-'}\n",
    "\n",
    "#FORMAT by YEAR, MONTH, DAY \n",
    "df1_tb1['dim calendar.date.1'] = df_tb1['dim calendar.date.1'].apply(lambda x: x.split(' de ')[-1] + months[x.split(' de ')[1]] + x.split(' de ')[0])\n",
    "\n",
    "#  Dim CalendarTo Datetime\n",
    "df1_tb1['dim calendar.date.1'] = pd.to_datetime(df_tb1['dim calendar.date.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need these Dim Columns once we already have the date and the time in Time Column\n",
    "df1_tb1 = df_tb1.drop(columns=['dim time.hour','dim time.minutes', 'dim time.time id'])\n",
    "# Columns with absolute 0's in our all the rows  can't aggregate anything\n",
    "df1_tb1 = df_tb1.drop(columns=['hold calls _ hsplit', 'hold time _ hsplit', 'i auxtime _ hsplit','time'])\n",
    "\n",
    "\n",
    "#Categorical variables\n",
    "df1_tb1['type'] = df1_tb1['type'].astype('category')\n",
    "df1_tb1['skill id'] = df1_tb1['skill id'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.2 *FACT HAGENT* Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Dtypes\n",
    "#Categoricals\n",
    "df1_tb2['short login'] = df2_tb2['short login'].astype('category') \n",
    "df1_tb2['short login'] = df1_tb2[df1_tb2['short login'] != 'ND#']\n",
    "df1_tb2['type'] = df1_tb2['type'].astype('category') \n",
    "df1_tb2['lob'] = df1_tb2['lob'].astype('category')\n",
    "\n",
    "# Calendar\n",
    "months = {'janeiro':'-01-', 'fevereiro':'-02-', 'março':'-03-', 'abril':'-04-', 'maio':'-05-', 'junho':'-06-', 'julho':'-07-',\n",
    "         'agosto': '-08-', 'setembro':'-09-', 'outubro':'-10-', 'novembro':'-11-', 'dezembro':'-12-'}\n",
    "\n",
    "df1_tb2['dim calendar.date.1'] = df1_tb2['dim calendar.date.1'].apply(lambda x: x.split(' de ')[-1] + months[x.split(' de ')[1]] + x.split(' de ')[0]) \n",
    "df1_tb2['dim calendar.date.1'] = pd.to_datetime(df1_tb2['dim calendar.date.1'], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets to datetime for transform in minutes after\n",
    "df1_tb2['contact holdtime'] = pd.to_datetime(unit='m', arg=df1_tb2['contact holdtime'])\n",
    "df1_tb2['contact talk time'] = pd.to_datetime(unit='m', arg=df1_tb2['contact talk time'])\n",
    "df1_tb2['idle time t'] = df1_tb2['idle time t']/60\n",
    "df1_tb2['login duration'] = pd.to_datetime(unit='m',arg=df1_tb2['login duration'])\n",
    "df1_tb2['post call processing time'] = pd.to_datetime(unit='m',arg=df1_tb2['post call processing time'])\n",
    "df1_tb2['ring time'] = pd.to_datetime(unit='m',arg=df1_tb2['ring time'])\n",
    "df1_tb2['schedule time'] = pd.to_datetime(unit='m',arg=df1_tb2['scheduled time'])\n",
    "df1_tb2['total staffed time'] = pd.to_datetime(unit='m',arg=df1_tb2['total staffed time'])\n",
    "df1_tb2['wait time'] = pd.to_datetime(unit='m',arg=df1_tb2['wait time'])\n",
    "df1_tb2['worked time'] = pd.to_datetime(unit='m',arg=df1_tb2['worked time'])\n",
    "df1_tb2['login duration'] = df1_tb2['login duration']/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming minutes\n",
    "df1_tb2['contact holdtime'] = df1_tb2['contact holdtime'].dt.minute\n",
    "df1_tb2['contact talk time'] = df1_tb2['contact talk time'].dt.minute\n",
    "df1_tb2['login duration'] = df1_tb2['login duration']/60\n",
    "df1_tb2['post call processing time'] = df1_tb2['post call processing time'].dt.minute\n",
    "df1_tb2['ring time'] = df1_tb2['ring time'].dt.minute\n",
    "df1_tb2['schedule time'] = df1_tb2['schedule time'].dt.minute\n",
    "df1_tb2['total staffed time'] =df1_tb2['total staffed time'].dt.minute\n",
    "df1_tb2['wait time'] = df1_tb2['wait time'].dt.minute\n",
    "df1_tb2['worked time'] = df1_tb2['worked time'].dt.minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.3 *FACT SERVREQ* Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set calendar time \n",
    "# Calendar\n",
    "months = {'janeiro':'-01-', 'fevereiro':'-02-', 'março':'-03-', 'abril':'-04-', 'maio':'-05-', 'junho':'-06-', 'julho':'-07-',\n",
    "         'agosto': '-08-', 'setembro':'-09-', 'outubro':'-10-', 'novembro':'-11-', 'dezembro':'-12-'}\n",
    "\n",
    "df1_tb3['dim calendar.date.1'] = df1_tb3['dim calendar.date.1'].apply(lambda x: x.split(' de ')[-1] + months[x.split(' de ')[1]] + x.split(' de ')[0]) \n",
    "df1_tb3['dim calendar.date.1'] = pd.to_datetime(df1_tb3['dim calendar.date.1'], format=\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "#Drops\n",
    "df1_tb3 = df1_tb3.drop(columns=['dim time.hour','dim time.minutes', 'dim time.time id','time'])\n",
    "\n",
    "#Categorical\n",
    "df1_tb3['dim agent.login'] = df1_tb3['dim agent.login'].astype('category')\n",
    "df1_tb3['resolution name'] = df1_tb3['resolution name'].astype('category')\n",
    "df1_tb3['tier'] = df1_tb3['tier'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.4 *FACT E-MAIL* Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date\n",
    "months = {'janeiro':'-01-', 'fevereiro':'-02-', 'março':'-03-', 'abril':'-04-', 'maio':'-05-', 'junho':'-06-', 'julho':'-07-',\n",
    "         'agosto': '-08-', 'setembro':'-09-', 'outubro':'-10-', 'novembro':'-11-', 'dezembro':'-12-'}\n",
    "\n",
    "df1_tb4['date'] = df1_tb4['date'].apply(lambda x: x.split(' de ')[-1] + months[x.split(' de ')[1]] + x.split(' de ')[0]) \n",
    "df1_tb4['date'] = pd.to_datetime(df1_tb4['date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "df1_tb4['agent login'] = df1_tb4['agent login'].astype('category')\n",
    "df1_tb4['agent login'] = df1_tb4['agent login'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.5 *CSAT* Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_sat = pd.read_excel('../data/Case_Study_Data.xlsb', engine='pyxlsb', sheet_name=4)\n",
    "df_sat = df_raw_sat.copy()\n",
    "#Strip \n",
    "df_sat.columns = df_sat.columns.\\\n",
    "    str.strip().str.lower().str.replace(' ', '_').str.replace('.', '').str.replace('-', '')\n",
    "df_sat.loc[:,'date'] = df_sat['date'].apply(lambda x: dateparser.parse(x))\n",
    "\n",
    "#Categorical\n",
    "df_sat['channel'] = df_sat['channel'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Analysis - ANSWER RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create a DataFrame for calculate the number of \n",
    "df_ans_rate = df_tb1[['dim calendar.date.1', 'answered calls', 'offered calls']].\\\n",
    "    groupby(by='dim calendar.date.1').\\\n",
    "    sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans_rate['answer rate'] = df_ans_rate.\\\n",
    "    apply(lambda x: x['answered calls']*100/x['offered calls'] if x['offered calls'] else 0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ans_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_ans_rate, x ='dim calendar.date.1', y='answer rate')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        *For Github Visualization, because the plot is dynamic*\n",
    "![image](../img/02_ANSWER_RATE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2.1 METRICS ANALYSIS -  CALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_call_status = df_tb2[['dim calendar.date.1',\n",
    "                            'calls abandoned', 'calls answered','calls conferenced', \n",
    "                            'calls return to q due to timeout','calls transferred', 'type']].\\\n",
    "    groupby(by=['dim calendar.date.1','type']).\\\n",
    "    sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 *FACT HAGENT* Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Analysis - CALLS\n",
    "        *Abandoned / Answered/ Confereced/ Transferred*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_call_status = df1_tb2[['dim calendar.date.1',\n",
    "                            'calls abandoned', 'calls answered','calls conferenced', \n",
    "                            'calls return to q due to timeout','calls transferred', 'type']].\\\n",
    "    groupby(by=['dim calendar.date.1','type']).\\\n",
    "    sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot this and see how it shows up \n",
    "\n",
    "fig1 = px.line(df_call_status, x='dim calendar.date.1', y=['calls abandoned', 'calls answered', \n",
    "                                                         'calls conferenced', 'calls return to q due to timeout',\n",
    "                                                         'calls transferred'], color='type')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            *For Github Visualization, because the plot is dynamic*\n",
    "![image](../img/03_CALL_ANALYSIS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Anaysis - SLA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this one we can use just 2 columns for get insights\n",
    "cols=['date','time since arrived to close']\n",
    "\n",
    "# At first I thought an interesting approuch would be to turn seconds into days. \n",
    "#This is because, through empirical experience and research, the SLA always has a value around\n",
    "#about 24 hours.\n",
    "\n",
    "df_tb4_s = df_tb4[cols]\n",
    "\n",
    "df_tb4['seconds_in_a_day'] = 24*60*60\n",
    "\n",
    "# The SLA we have in the table is related to the agents, at this moment my intention is to create an SLA satisfaction index, \n",
    "#using as a reference the 24 hours to solve the problem, that is, compliance with the\n",
    "#Service Level Agreement.\n",
    "df_tb4['sla_lower_24h'] = df_tb4.apply(lambda df: 1 if df['time since arrived to close'] < df['seconds_in_a_day'] else 0, axis=1)\n",
    "\n",
    "#Total Tickets \n",
    "df_tb4['total_tickets'] = df_tb4.groupby(['date'])['date'].transform('count')\n",
    "\n",
    "\n",
    "df_tb4[df_tb4['sla_lower_24h'] ==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We can see a high number of tickets! For now, the correct thing would be to perform an agribusiness by the date and total tickets in the face of the SLA less than 24.\n",
    "\n",
    "  ##### The point here is to seek customer satisfaction  by complying with SLA in a short time\n",
    "\n",
    " #### E-mail SLA METRICS\n",
    "     will be given in the newly created column (SLA) and in this dataframe we will be able to visualize and get a sense of what happens over time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb4_grouped = df_tb4.groupby(['date','total_tickets'])['sla_lower_24h'].sum().reset_index()\n",
    "df_tb4_grouped['sla']=df_tb4_grouped['sla_lower_24h']/df_tb4_grouped['total_tickets']*100\n",
    "\n",
    "df_tb4_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.line(df_tb4_grouped, x='date', y='sla')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Github Visualization, because the plot is dynamic*\n",
    "![image](../img/04_SLA_EMAIL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics Analysis - CSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSAT \n",
    "df_raw_sat = pd.read_excel('../data/Case_Study_Data.xlsb', engine='pyxlsb', sheet_name=4)\n",
    "df_sat = df_raw_sat.copy()\n",
    "#Strip \n",
    "df_sat.columns = df_sat.columns.\\\n",
    "    str.strip().str.lower().str.replace(' ', '_').str.replace('.', '').str.replace('-', '')\n",
    "df_sat.loc[:,'date'] = df_sat['date'].apply(lambda x: dateparser.parse(x))\n",
    "\n",
    "#Categorical\n",
    "df_sat['channel'] = df_sat['channel'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
